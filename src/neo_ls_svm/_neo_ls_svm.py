"""Neo LS-SVM."""

from typing import Any, Literal, TypeVar, cast

import numpy as np
import numpy.typing as npt
from scipy.linalg import eigh, lu_factor, lu_solve
from sklearn.base import BaseEstimator, clone
from sklearn.isotonic import IsotonicRegression
from sklearn.metrics import accuracy_score, r2_score
from sklearn.metrics.pairwise import euclidean_distances, rbf_kernel
from sklearn.utils.validation import check_consistent_length, check_X_y

from neo_ls_svm._affine_feature_map import AffineFeatureMap
from neo_ls_svm._affine_separator import AffineSeparator
from neo_ls_svm._feature_maps import (
    KernelApproximatingFeatureMap,
    OrthogonalRandomFourierFeatures,
)
from neo_ls_svm._typing import (
    ComplexMatrix,
    ComplexVector,
    FloatMatrix,
    FloatVector,
    GenericVector,
)

T = TypeVar("T", np.complex64, np.complex128)
F = TypeVar("F", np.float32, np.float64)


class NeoLSSVM(BaseEstimator):
    """Neo LS-SVM.

    A neo Least-Squares Support Vector Machine with:

      - [x] A next-generation regularisation term that penalises the complexity of the prediction
            surface, decision function, and maximises the margin.
      - [x] Large-scale support through state-of-the-art random feature maps.
      - [x] Optional automatic selection of primal or dual problem.
      - [x] Automatic optimal tuning of the regularisation hyperparameter Î³ that minimises the
            leave-one-out error, without having to refit the model.
      - [x] Automatic tuning of the kernel parameters Ïƒ, without having to refit the model.
      - [x] Automatic robust shift and scaling of the feature matrix and labels.
      - [x] Leave-one-out residuals, leverage, influence, and error as a free output after fitting,
            optimally clipped in classification.
      - [x] Isotonically calibrated class probabilities based on leave-one-out predictions.
      - [ ] Automatic robust fit by removing outliers.
    """

    def __init__(  # noqa: PLR0913
        self,
        *,
        primal_feature_map: KernelApproximatingFeatureMap | None = None,
        dual_feature_map: AffineSeparator | None = None,
        dual: bool | None = False,
        max_epochs: int = 1,
        refit: bool = False,
        random_state: int | np.random.RandomState | None = 42,
        estimator_type: Literal["classifier", "regressor"] | None = None,
    ) -> None:
        self.primal_feature_map = primal_feature_map
        self.dual_feature_map = dual_feature_map
        self.dual = dual
        self.max_epochs = max_epochs
        self.refit = refit
        self.random_state = random_state
        self.estimator_type = estimator_type

    def _optimize_Î²Ì‚_Î³(
        self,
        C: FloatMatrix[F],
        Ï†: ComplexMatrix[T],
        y: FloatVector[F],
        s: FloatVector[F],
    ) -> tuple[ComplexVector[T], float, ComplexMatrix[T]]:
        """Find Î²Ì‚ = argmin ||S(Ï†(X)Î²Ì‚ - y)||Â² + Î³Î²Ì‚'CÎ²Ì‚ and Î³ that minimises the leave-one-out error.

        First, we solve min ||S(Ï†(X)Î²Ì‚ - y)||Â² + Î³Î²Ì‚'CÎ²Ì‚ for Î²Ì‚ as a function of Î³::

          (Î³C + Ï†(X)'SSÏ†(X)) Î²Ì‚ = Ï†(X)'S Sy
          (Î³I + Câ»Â¹Ï†(X)'SSÏ†(X)) Î²Ì‚ = Câ»Â¹Ï†(X)'S Sy
          (yQQâ»Â¹ + QÎ›Qâ»Â¹) Î²Ì‚ = Câ»Â¹Ï†(X)'S Sy  where  Ï†(X)'SSÏ†(X)Q = CQÎ›
          Î²Ì‚ = Q(Î³I + Î›)â»Â¹Qâ»Â¹ Câ»Â¹Ï†(X)'S Sy

        The entries of Î²Ì‚ are rational polynomials of Î³: Q diag([ráµ¢(Î³)]áµ¢) Qâ»Â¹ Câ»Â¹Ï†(X)'SSy. The
        weighted leave-one-out residuals eáµ¢ are given by [1]::

          eáµ¢ := sáµ¢(Ï†(X)áµ¢Î²Ì‚ - yáµ¢) / (1 - háµ¢)
          háµ¢ := sáµ¢Ï†(X)áµ¢ (Î³C + Ï†(X)'SSÏ†(X))â»Â¹ sáµ¢Ï†(X)áµ¢'  where  Ï†(X)áµ¢ is the i'th row of Ï†(X)
              = sáµ¢Ï†(X)áµ¢ Q(Î³I + Î›)â»Â¹Qâ»Â¹Câ»Â¹    sáµ¢Ï†(X)áµ¢'

        The entries of háµ¢ are also rational polynomials of Î³: sáµ¢Ï†(X)áµ¢Q diag([ráµ¢(Î³)]áµ¢) Qâ»Â¹Câ»Â¹Ï†(X)áµ¢'.

        We find the Î³ that optimises the weighted mean absolute leave-one-out error (WMAE) s'abs(e)
        by sampling sufficient Î³s and picking the best one.

        References
        ----------
        [1] https://robjhyndman.com/hyndsight/loocv-linear-models/
        """
        # Compute the GEVD Ï†(X)'SSÏ†(X)Q = CQÎ› so that QÎ›Qâ»Â¹ = Câ»Â¹Ï†(X)'SSÏ†(X).
        SÏ† = s[:, np.newaxis] * Ï†
        A = SÏ†.conj().T @ SÏ†
        A = (A + A.conj().T) / 2  # Ensure A is fully Hermitian.
        c = np.diag(C)
        C_is_diagonal = np.all(np.diag(c) == C)
        if C_is_diagonal:
            Î», Q = eigh((1 / c[:, np.newaxis]) * A)  # Scipy's eigh is faster for complex matrices.
            CQ_inv = Q.conj().T * (1 / c[np.newaxis, :])
        else:
            Î», Q = eigh(a=A, b=C)
            CQ_lu = lu_factor(C @ Q)
        # Compute the optimal parameters Î²Ì‚ = Q(Î³I + Î›)â»Â¹Qâ»Â¹Câ»Â¹Ï†(X)'SSy as a function of Î³. We can
        # evaluate Î²Ì‚(Î³) as Î²Ì‚ @ (1 / (Î³ + Î»)) for a given Î³.
        Ï†STSy = SÏ†.conj().T @ (s * y)
        Î²Ì‚ = (
            Q * (CQ_inv @ Ï†STSy)[np.newaxis, :]
            if C_is_diagonal
            else Q * lu_solve(CQ_lu, Ï†STSy)[np.newaxis, :]
        )
        # Compute part of the leave-one-out residual numerator sáµ¢(yáµ¢ - Ï†(X)áµ¢Î²Ì‚).
        Ï†Î²Ì‚ = np.real(Ï† @ Î²Ì‚)
        # Compute the leverage part háµ¢ of the leave-one-out residual denominator 1 - háµ¢.
        h = (
            np.real(SÏ† @ Q * (CQ_inv @ SÏ†.conj().T).T)
            if C_is_diagonal
            else np.real(SÏ† @ Q * lu_solve(CQ_lu, SÏ†.conj().T).T)
        )
        # After np.real, arrays are _neither_ C nor F contiguous, which destroys performance.
        Ï†Î²Ì‚ = np.ascontiguousarray(Ï†Î²Ì‚)
        h = np.ascontiguousarray(h)
        # Evaluate the unweighted leave-one-out residuals for a set of Î³s with two matrix-matrix
        # products and pick the best solution.
        complexity_weights = np.logspace(np.log10(1e-6), np.log10(0.9), 1024, dtype=y.dtype)
        error_weights = 1 - complexity_weights
        self.Î³s_: FloatVector[F] = complexity_weights / error_weights
        rÎ³ = 1 / (self.Î³s_[np.newaxis, :] + Î»[:, np.newaxis])
        with np.errstate(divide="ignore", invalid="ignore"):
            unweighted_loo_residuals = (Ï†Î²Ì‚ @ rÎ³ - y[:, np.newaxis]) / (1 - h @ rÎ³)
            loo_residuals = self.y_scale_ * unweighted_loo_residuals
        # In the case of binary classification, clip overly positive and overly negative
        # predictions' residuals to 0 when the labels are positive and negative, respectively.
        if self._estimator_type == "classifier":
            loo_residuals[(y > 0)[:, np.newaxis] & (loo_residuals > 0)] = 0
            loo_residuals[(y < 0)[:, np.newaxis] & (loo_residuals < 0)] = 0
        # Select y that minimises the number of LOO misclassifications, the degree to which
        # LOO instances are misclassified, and the weighted absolute LOO error.
        self.loo_errors_Î³s_ = (s**2) @ np.abs(loo_residuals)
        optimum = np.argmin(
            (s**2) @ (np.abs(loo_residuals) >= 1)
            + (s**2) @ np.maximum(0, np.abs(loo_residuals) - 1)
            + self.loo_errors_Î³s_
            if self._estimator_type == "classifier"
            else self.loo_errors_Î³s_
        )
        # Store the leave-one-out residuals, leverage, influence, error, and score.
        self.loo_residuals_ = loo_residuals[:, optimum]
        self.loo_leverage_ = h @ rÎ³[:, optimum]
        dfbetas = (  # := Î²Ì‚â‚áµ¢â‚Ž - Î²Ì‚
            ((Q @ (rÎ³[:, optimum] * Q.conj().T)) @ SÏ†.conj().T)
            * s[np.newaxis, :]
            * unweighted_loo_residuals[:, optimum][np.newaxis, :]
        )
        self.loo_error_ = self.loo_errors_Î³s_[optimum]
        if self._estimator_type == "classifier":
            # Compute the score as a weighted leave-one-out accuracy.
            self.loo_score_ = 1.0 - (s**2) @ (np.abs(self.loo_residuals_) > 1)
        elif self._estimator_type == "regressor":
            # Compute the score as a weighted leave-one-out RÂ².
            È³ = (s**2) @ y
            Sy, SÈ³ = self.y_scale_ * s * y, self.y_scale_ * s * È³
            denom = (Sy - SÈ³) @ (Sy - SÈ³)
            numer = (s * self.loo_residuals_) @ (s * self.loo_residuals_)
            self.loo_score_ = 1.0 - numer / denom if denom > 0 else 1.0
        Î²Ì‚, Î³ = Î²Ì‚ @ rÎ³[:, optimum], self.Î³s_[optimum]
        # Resolve the linear system for better accuracy.
        if self.refit:
            Î²Ì‚ = np.linalg.solve(Î³ * C + A, Ï†STSy)
        self.residuals_ = np.real(Ï† @ Î²Ì‚) - y
        if self._estimator_type == "classifier":
            self.residuals_[(y > 0) & (self.residuals_ > 0)] = 0
            self.residuals_[(y < 0) & (self.residuals_ < 0)] = 0
        # TODO: Print warning if optimal Î³ is found at the edge.
        return Î²Ì‚, Î³, dfbetas

    def _optimize_Î±Ì‚_Î³(  # noqa: PLR0913
        self,
        C: FloatMatrix[F],
        X: FloatMatrix[F],
        y: FloatVector[F],
        s: FloatVector[F],
        gamma: float = 0.5,
    ) -> tuple[FloatVector[F], float]:
        """Find the dual solution to argmin ||S(Ï†(X)Î²Ì‚ - y)||Â² + Î³Î²Ì‚'CÎ²Ì‚ that minimises the LOO error.

        The solution to argmin ||S(Ï†(X)Î²Ì‚ - y)||Â² + Î³Î²Ì‚'CÎ²Ì‚ for Î²Ì‚ as a function of Î±Ì‚ is:

          (Î³C + Ï†(X)'SSÏ†(X))Î²Ì‚ = Ï†(X)'S Sy
          (Î³ð•€ + Câ»Â¹Ï†(X)'SSÏ†(X))Î²Ì‚ = Câ»Â¹Ï†(X)'S Sy
          Î²Ì‚ = (Î³ð•€ + Câ»Â¹Ï†(X)'SSÏ†(X))â»Â¹ Câ»Â¹Ï†(X)'S Sy
          Î²Ì‚ = Câ»Â¹Ï†(X)'S (Î³ð•€ + SÏ†(X)Câ»Â¹Ï†(X)'S)â»Â¹ Sy  with the identity  (Î³ð•€ + AB)â»Â¹A = A(Î³ð•€ + BA)â»Â¹
          Î²Ì‚ = Câ»Â¹Ï†(X)' (Î³Sâ»Â² + Ï†(X)Câ»Â¹Ï†(X)')â»Â¹ y
          Î²Ì‚ = Câ»Â¹Ï†(X)' Î±Ì‚  where Î±Ì‚ := (Î³Sâ»Â² + Ï†(X)Câ»Â¹Ï†(X)')â»Â¹ y

        Let's now make two modifications to the dual solution. First we'll regularise Î±Ì‚ directly
        with Î³Î±Ì‚'CÎ±Ì‚ instead of regularising Î²Ì‚, yielding:

          Î²Ì‚ := Ï†(X)' Î±Ì‚
          Î±Ì‚ := (Î³Sâ»Â¹CSâ»Â¹ + Ï†(X)Ï†(X)')â»Â¹ y = (Î³Sâ»Â¹CSâ»Â¹ + k(X, X))â»Â¹ y
          Å·(x) := Ï†(x)'Î²Ì‚ = k(x, X)Î±Ì‚

        Next we'll add a bias term to the prediction function in the dual space:

          Î±Ì‚ := (Î³[Sâ»Â¹CSâ»Â¹, 0; 0, 1] + [k(X, X), 1; 1', 0])â»Â¹ [y; 0] := (Î³D + K)â»Â¹ [y; 0]
          Å·(x) := [k(x, X) 1] Î±Ì‚

        Now we can solve for Î±Ì‚ as a function of Î³:

          (Î³D + K) Î±Ì‚ = [y; 0]
          (Î³ð•€ + Dâ»Â¹K) Î±Ì‚ = Dâ»Â¹ [y; 0]
          (Î³QQâ»Â¹ + QÎ›Qâ»Â¹) Î±Ì‚ = Dâ»Â¹ [y; 0]  where  KQ = DQÎ›
          Î±Ì‚ = Q(Î³ð•€ + Î›)â»Â¹Qâ»Â¹Dâ»Â¹ [y; 0]

        The entries of Î±Ì‚ are rational polynomials of Î³: Q diag([ráµ¢(Î³)]áµ¢) Qâ»Â¹Dâ»Â¹ [y; 0]. The weighted
        leave-one-out residuals eáµ¢ can be derived by analogy to [1]::

          eáµ¢ := sáµ¢(Å·â½â»áµâ¾(xáµ¢) - yáµ¢)
             = -sáµ¢[Î±Ì‚áµ¢ / (Î³D + K)â»Â¹áµ¢áµ¢ +
                   Î³(Sâ»Â¹CSâ»Â¹ - diag(Sâ»Â¹CSâ»Â¹))áµ¢â‚Œ (Î±Ì‚ - (Î³D + K)â»Â¹â‚Œáµ¢ / (Î³D + K)â»Â¹áµ¢áµ¢)]
             = -sáµ¢[Î±Ì‚áµ¢ / (Q(Î³ð•€ + Î›)â»Â¹Qâ»Â¹Dâ»Â¹)áµ¢áµ¢ + Î³(Sâ»Â¹CSâ»Â¹ - diag(Sâ»Â¹CSâ»Â¹))áµ¢â‚Œ
                  (Î±Ì‚ - (Q(Î³ð•€ + Î›)â»Â¹Qâ»Â¹Dâ»Â¹)â‚Œáµ¢ / (Q(Î³ð•€ + Î›)â»Â¹Qâ»Â¹Dâ»Â¹)áµ¢áµ¢)]

        The entries of eáµ¢ are also rational polynomials of Î³.

        We find the Î³ that optimises the weighted mean absolute leave-one-out error (WMAE) s'abs(e)
        by sampling sufficient Î³s and picking the best one.

        References
        ----------
        [1] http://theoval.cmp.uea.ac.uk/publications/pdf/ijcnn2006a.pdf
        """
        # Construct D := [Sâ»Â¹CSâ»Â¹, 0; 0, 1] and K := [k(X, X), 1; 1', 0].
        D = np.zeros((X.shape[0] + 1, X.shape[0] + 1), dtype=X.dtype)
        D[:-1, :-1] = (C / s[:, np.newaxis]) / s[np.newaxis, :]
        D[-1, -1] = np.mean(np.diag(D[:-1, :-1]))
        K = np.ones((X.shape[0] + 1, X.shape[0] + 1), dtype=X.dtype)
        K[:-1, :-1] = rbf_kernel(X, gamma=gamma)
        K[-1, -1] = 0
        # Compute the GEVD KQ = DQÎ› so that QÎ›Qâ»Â¹ = Dâ»Â¹K.
        d = np.diag(D)
        D_is_diagonal = np.all(np.diag(d) == D)
        if D_is_diagonal:
            Î», Q = eigh((1 / d[:, np.newaxis]) * K)  # Scipy's eigh is faster for complex matrices.
            DQ_inv = Q.conj().T * (1 / d[np.newaxis, :])
        else:
            Î», Q = eigh(a=K, b=D)
            DQ_lu = lu_factor(D @ Q)
            DQ_inv = lu_solve(DQ_lu, np.eye(D.shape[0], dtype=D.dtype))
        # Compute the optimal parameters aÌ‚ = Q(Î³I + Î›)â»Â¹Qâ»Â¹Dâ»Â¹ [y; 0] as a function of Î³. We can
        # evaluate aÌ‚(Î³) as aÌ‚ @ (1 / (Î³ + Î»)) for a given Î³.
        y_zero = np.append(y, 0).astype(y.dtype)
        Î±Ì‚ = (
            Q * (DQ_inv @ y_zero)[np.newaxis, :]
            if D_is_diagonal
            else Q * lu_solve(DQ_lu, y_zero)[np.newaxis, :]
        )
        # Compute hâ‚– := (Q(Î³I + Î›)â»Â¹Qâ»Â¹Dâ»Â¹)â‚–â‚– as a function of Î³. We can evaluate h(Î³) as
        # h @ (1 / (Î³ + Î»)) for a given Î³.
        h = Q * DQ_inv.T
        # Compute gâ‚– := (Sâ»Â¹CSâ»Â¹ - diag(Sâ»Â¹CSâ»Â¹))â‚–_.
        G = D[:-1, :-1].copy()
        np.fill_diagonal(G, 0)
        # Evaluate the unweighted leave-one-out residuals for a set of Î³s and pick the best one.
        complexity_weights = np.logspace(
            np.log10(1e-6), np.log10(0.9), 1024 if D_is_diagonal else 32, dtype=X.dtype
        )
        error_weights = 1 - complexity_weights
        self.Î³s_ = complexity_weights / error_weights
        rÎ³ = 1 / (self.Î³s_[np.newaxis, :] + Î»[:, np.newaxis])
        loo_residuals = (
            -self.y_scale_ * np.real(Î±Ì‚[:-1, :] @ rÎ³) / np.real(h[:-1, :] @ rÎ³)
            if D_is_diagonal
            else -self.y_scale_
            * (
                np.real(Î±Ì‚[:-1, :] @ rÎ³) / np.real(h[:-1, :] @ rÎ³)
                + self.Î³s_[np.newaxis, :] * (G @ np.real(Î±Ì‚[:-1, :] @ rÎ³))
                - self.Î³s_[np.newaxis, :]
                / np.real(h[:-1, :] @ rÎ³)
                * np.einsum(
                    "ji,ir,rk,rj->jk",
                    G,
                    Q[:-1, :-1],
                    rÎ³[:-1, :],
                    DQ_inv[:-1, :-1],
                    optimize="optimal",
                )
            )
        )
        # In the case of binary classification, clip overly positive and overly negative
        # predictions' residuals to 0 when the labels are positive and negative, respectively.
        if self._estimator_type == "classifier":
            loo_residuals[(y > 0)[:, np.newaxis] & (loo_residuals > 0)] = 0
            loo_residuals[(y < 0)[:, np.newaxis] & (loo_residuals < 0)] = 0
        # Select Î³ that minimises the number of LOO misclassifications, the degree to which
        # LOO instances are misclassified, and the weighted absolute LOO error.
        self.loo_errors_Î³s_ = (s**2) @ np.abs(loo_residuals)
        optimum = np.argmin(
            (s**2) @ (np.abs(loo_residuals) >= 1)
            + (s**2) @ np.maximum(0, np.abs(loo_residuals) - 1)
            + self.loo_errors_Î³s_
            if self._estimator_type == "classifier"
            else self.loo_errors_Î³s_
        )
        # Store the leave-one-out residuals, leverage, error, and score.
        self.loo_residuals_ = loo_residuals[:, optimum]
        self.loo_leverage_ = 1.0 - np.real(h[:-1, :] @ rÎ³[:, optimum])
        self.loo_error_ = self.loo_errors_Î³s_[optimum]
        if self._estimator_type == "classifier":
            # Compute the score as a weighted leave-one-out accuracy.
            self.loo_score_ = 1.0 - (s**2) @ (np.abs(self.loo_residuals_) > 1)
        elif self._estimator_type == "regressor":
            # Compute the score as a weighted leave-one-out RÂ².
            È³ = (s**2) @ y
            Sy, SÈ³ = self.y_scale_ * s * y, self.y_scale_ * s * È³
            denom = (Sy - SÈ³) @ (Sy - SÈ³)
            numer = (s * self.loo_residuals_) @ (s * self.loo_residuals_)
            self.loo_score_ = 1.0 - numer / denom if denom > 0 else 1.0
        Î±Ì‚, Î³ = np.real(Î±Ì‚ @ rÎ³[:, optimum]), self.Î³s_[optimum]
        # Resolve the linear system for better accuracy.
        if self.refit:
            Î±Ì‚ = np.linalg.solve(Î³ * D + K, y_zero)
        self.residuals_ = (K[:-1, :-1] @ self.Î±Ì‚_[:-1] + self.Î±Ì‚_[-1]) - y
        if self._estimator_type == "classifier":
            self.residuals_[(y > 0) & (self.residuals_ > 0)] = 0
            self.residuals_[(y < 0) & (self.residuals_ < 0)] = 0
        # TODO: Print warning if optimal Î³ is found at the edge.
        return Î±Ì‚, Î³

    def fit(  # noqa: C901, PLR0912, PLR0915
        self, X: FloatMatrix[F], y: GenericVector, sample_weight: FloatVector[F] | None = None
    ) -> "NeoLSSVM":
        """Fit this predictor."""
        # Remove singleton dimensions from y and validate input.
        X, y = check_X_y(X, y, dtype=(np.float64, np.float32))
        y = np.ravel(np.asarray(y))
        self.n_features_in_ = X.shape[1]
        # Use uniform sample weights if none are provided.
        sample_weight_ = (
            np.ones(y.shape, X.dtype)
            if sample_weight is None
            else np.ravel(np.asarray(sample_weight))
        )
        check_consistent_length(y, sample_weight_)
        # Store the target's dtype for prediction.
        self.y_dtype_: npt.DTypeLike = y.dtype
        # Learn the type of task from the target. Set the `_estimator_type` and store the `classes_`
        # attribute if this is a classification task [1].
        # [1] https://scikit-learn.org/stable/glossary.html#term-classifiers
        y_: FloatVector[F]
        unique_y = np.unique(y)
        inferred_estimator_type = None
        if len(unique_y) == 2:  # noqa: PLR2004
            inferred_estimator_type = "classifier"
        elif (
            np.issubdtype(y.dtype, np.number)
            or np.issubdtype(y.dtype, np.datetime64)
            or np.issubdtype(y.dtype, np.timedelta64)
        ):
            inferred_estimator_type = "regressor"
        self._estimator_type: str | None = self.estimator_type or inferred_estimator_type
        if self._estimator_type == "classifier":
            self.classes_: GenericVector = unique_y
            negatives = y == self.classes_[0]
            y_ = np.ones(y.shape, dtype=X.dtype)
            y_[negatives] = -1
        elif self._estimator_type == "regressor":
            y_ = cast(npt.NDArray[np.floating[Any]], y)
        else:
            message = "Target type not supported"
            raise ValueError(message)
        # Fit robust shift and scale parameters for the target y.
        if self._estimator_type == "classifier":
            self.y_shift_: float = 0.0
            self.y_scale_: float = 1.0
        elif self._estimator_type == "regressor":
            l, self.y_shift_, u = np.quantile(y_, [0.05, 0.5, 0.95])  # noqa: E741
            self.y_scale_ = np.maximum(np.abs(l - self.y_shift_), np.abs(u - self.y_shift_))
        self.y_scale_ = 1.0 if self.y_scale_ <= np.finfo(X.dtype).eps else self.y_scale_
        y_ = ((y_ - self.y_shift_) / self.y_scale_).astype(X.dtype)
        # Initialise the primal and dual feature maps.
        self.primal_feature_map_ = clone(
            self.primal_feature_map or OrthogonalRandomFourierFeatures()
        )
        self.dual_feature_map_ = clone(self.dual_feature_map or AffineSeparator())
        # Determine whether we want to solve this in the primal or dual space.
        self.dual_ = X.shape[0] <= 768 if self.dual is None else self.dual  # noqa: PLR2004
        self.primal_ = not self.dual_
        # Learn an optimal distance metric for the primal or dual space and apply it to the feature
        # matrix X.
        if self.primal_:
            self.primal_feature_map_.fit(X, y_, sample_weight_)
            X = self.primal_feature_map_.transform(X)
        else:
            self.dual_feature_map_.fit(X, y_, sample_weight_)
            X = self.dual_feature_map_.transform(X)
        # Optimise the following sub-objectives for the weights Î²Ì‚ and hyperparameter Î³:
        # 1. Minimal mean squared error on training set: ||S(y - Ï†(X)Î²Ì‚)||Â²
        # 2. Minimal complexity of the prediction surface: âˆ«||âˆ‡â‚“Ï†(x)'Î²Ì‚||Â²dx
        # 3. Maximal margin: ||Î²Ì‚||Â².
        C: FloatMatrix[F]
        if self.primal_:
            C = self.primal_feature_map_.complexity_matrix.astype(X.dtype)
        else:
            gamma = 0.5
            C = np.sqrt(rbf_kernel(X, gamma=gamma)) * (
                1 - euclidean_distances(X, squared=True) * (gamma / X.shape[1])
            )
        # Combine sub-objectives (2) and (3) in a single regularisation matrix C.
        complexity_weight = 0.1
        margin_weight = 1 - complexity_weight
        C = complexity_weight * C + margin_weight * np.eye(C.shape[0], dtype=C.dtype)
        # Normalise C so that Î²Ì‚'CÎ²Ì‚ is comparable in magnitude to the training error (1).
        C /= C.shape[0]
        # Fit a robust model by iteratively removing outlÎ³ing examples.
        for i in range(self.max_epochs):
            # Remove outlÎ³ing examples.
            if i > 0:
                # TODO: Add algorithm that reweights or removes outliers.
                keep = np.ones(y.shape, dtype=np.bool_)
                if np.all(keep):
                    break
                X, y_, sample_weight_ = X[keep, :], y_[keep], sample_weight_[keep]
                if self.dual_:
                    C = C[keep, :]
                    C = C[:, keep]
            # Normalise the sample weights.
            sample_weight_ = sample_weight_ / np.sum(sample_weight_)
            s = np.sqrt(sample_weight_)
            # Solve the primal or dual system for y that minimises the leave-one-out error.
            if self.primal_:
                self.Î²Ì‚_, self.Î³_, dfbetas = self._optimize_Î²Ì‚_Î³(C=C, Ï†=X, y=y_, s=s)  # type: ignore[type-var,var-annotated]
            else:
                self.Î±Ì‚_, self.Î³_ = self._optimize_Î±Ì‚_Î³(C=C, X=X, y=y_, s=s, gamma=gamma)
        # Calibrate probabilities with isotonic regression on the leave-one-out predictions.
        if self._estimator_type == "classifier":
            self.predict_proba_calibrator_ = IsotonicRegression(
                out_of_bounds="clip", y_min=0, y_max=1, increasing=True
            )
            loo_Å· = self.loo_residuals_ + y_
            target = np.zeros_like(y_)
            target[y_ == np.max(y_)] = 1.0
            self.predict_proba_calibrator_.fit(loo_Å·, target, sample_weight_)
        return self

    def decision_function(self, X: FloatMatrix[F]) -> FloatVector[F]:
        """Evaluate this predictor's decision function."""
        Å·: FloatVector[F]
        if self.primal_:
            # Apply the feature map Ï† and predict as Å·(x) := Ï†(x)'Î²Ì‚.
            Å· = np.real(
                cast(KernelApproximatingFeatureMap, self.primal_feature_map_).transform(X) @ self.Î²Ì‚_
            )
        else:
            # Shift and scale X, then predict as Å·(x) := [k(x, X) 1] aÌ‚.
            X = cast(AffineFeatureMap, self.dual_feature_map_).transform(X)
            K = rbf_kernel(X, cast(FloatMatrix[F], self.X_), gamma=0.5)
            Å· = K @ self.Î±Ì‚_[:-1] + self.Î±Ì‚_[-1]
        return Å·

    def predict(self, X: FloatMatrix[F]) -> GenericVector:
        """Predict the output on a given dataset."""
        # Evaluate Å· given the feature matrix X.
        Å·_df = self.decision_function(X)
        if self._estimator_type == "classifier":
            # For binary classification, round to the nearest class label. When the decision
            # function is 0, we assign a negative class label [1].
            # [1] https://scikit-learn.org/stable/glossary.html#term-decision_function
            Å·_df = np.sign(Å·_df)
            Å·_df[Å·_df == 0] = -1
            # Remap to the original class labels.
            Å· = self.classes_[((Å·_df + 1) // 2).astype(np.intp)]
        elif self._estimator_type == "regressor":
            # Undo the label shift and scale.
            Å· = Å·_df.astype(np.float64) * self.y_scale_ + self.y_shift_
        # Map back to the training target dtype.
        Å· = Å·.astype(self.y_dtype_)
        return Å·

    def predict_proba(self, X: FloatMatrix[F]) -> FloatMatrix[F]:
        """Predict the output probability (classification) or confidence interval (regression)."""
        if self._estimator_type == "classifier":
            Å·_classification = self.decision_function(X)
            p = self.predict_proba_calibrator_.transform(Å·_classification)
            P = np.hstack([1 - p[:, np.newaxis], p[:, np.newaxis]])
        else:
            # TODO: Replace point predictions with confidence interval.
            Å·_regression = self.predict(X)
            P = np.hstack((Å·_regression[:, np.newaxis], Å·_regression[:, np.newaxis]))
        return P

    @property
    def loo_score(self) -> float:
        """Compute the leave-one-out score of this classifier or regressor."""
        return cast(float, self.loo_score_)

    def score(
        self, X: FloatMatrix[F], y: GenericVector, sample_weight: FloatVector[F] | None = None
    ) -> float:
        """Compute the accuracy or RÂ² of this classifier or regressor."""
        Å· = self.predict(X)
        score: float
        if self._estimator_type == "classifier":
            score = accuracy_score(y, Å·, sample_weight=sample_weight)
        elif self._estimator_type == "regressor":
            # Cast to a numeric dtype in case the target is a datetime or timedelta.
            score = r2_score(
                y.astype(np.float64), Å·.astype(np.float64), sample_weight=sample_weight
            )
        return score

    def _more_tags(self) -> dict[str, Any]:
        # https://scikit-learn.org/stable/developers/develop.html#estimator-tags
        return {"binary_only": True, "requires_y": True}
