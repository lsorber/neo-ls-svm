"""Neo LS-SVM."""

from typing import TYPE_CHECKING, Any, Literal, TypeVar, cast, overload

import numpy as np
import numpy.typing as npt
from scipy.linalg import cho_factor, cho_solve, eigh, lu_factor, lu_solve
from sklearn.base import BaseEstimator, clone
from sklearn.isotonic import IsotonicRegression
from sklearn.metrics import accuracy_score, r2_score
from sklearn.metrics.pairwise import euclidean_distances, rbf_kernel
from sklearn.model_selection import train_test_split
from sklearn.utils.validation import (
    check_array,
    check_consistent_length,
    check_is_fitted,
    check_X_y,
)

from neo_ls_svm._affine_feature_map import AffineFeatureMap
from neo_ls_svm._affine_separator import AffineSeparator
from neo_ls_svm._coherent_linear_quantile_regressor import CoherentLinearQuantileRegressor
from neo_ls_svm._feature_maps import (
    KernelApproximatingFeatureMap,
    OrthogonalRandomFourierFeatures,
)
from neo_ls_svm._typing import (
    ComplexMatrix,
    ComplexVector,
    FloatMatrix,
    FloatTensor,
    FloatVector,
    GenericVector,
)

if TYPE_CHECKING:
    import pandas as pd

C = TypeVar("C", np.complex64, np.complex128)
F = TypeVar("F", np.float32, np.float64)


class NeoLSSVM(BaseEstimator):
    """Neo LS-SVM.

    A neo Least-Squares Support Vector Machine with:

        1. ‚ö° Linear complexity in the number of training examples with Orthogonal Random Features.
        2. üöÄ Hyperparameter free: zero-cost optimization of the regularisation parameter Œ≥ and
             kernel parameter œÉ.
        3. üèîÔ∏è Adds a new tertiary objective that minimizes the complexity of the prediction surface.
        4. üéÅ Returns the leave-one-out residuals and error for free after fitting.
        5. üåÄ Learns an affine transformation of the feature matrix to optimally separate the
             target's bins.
        6. ü™û Can solve the LS-SVM both in the primal and dual space.
        7. üå°Ô∏è Isotonically calibrated `predict_proba`.
        8. ‚úÖ Conformally calibrated `predict_quantiles` and `predict_interval`.
        9. üîî Bayesian estimation of the predictive standard deviation with `predict_std`.
        10. üêº Pandas DataFrame output when the input is a pandas DataFrame.
    """

    def __init__(  # noqa: PLR0913
        self,
        *,
        primal_feature_map: KernelApproximatingFeatureMap | Literal["auto"] = "auto",
        dual_feature_map: AffineSeparator | Literal["auto"] = "auto",
        dual: bool | Literal["auto"] = "auto",
        estimator_type: Literal["auto", "classifier", "regressor"] = "auto",
        random_state: int | np.random.RandomState | None = 42,
    ) -> None:
        self.primal_feature_map = primal_feature_map
        self.dual_feature_map = dual_feature_map
        self.dual = dual
        self.random_state = random_state
        self.estimator_type = estimator_type

    def _optimize_Œ≤ÃÇ_Œ≥(
        self,
        œÜ: ComplexMatrix[C],
        y: FloatVector[F],
        s: FloatVector[F],
        C: FloatMatrix[F],
    ) -> tuple[ComplexVector[C], float]:
        """Find Œ≤ÃÇ = argmin ||S(œÜ(X)Œ≤ÃÇ - y)||¬≤ + Œ≥Œ≤ÃÇ'CŒ≤ÃÇ and Œ≥ that minimises the leave-one-out error.

        First, we solve min ||S(œÜ(X)Œ≤ÃÇ - y)||¬≤ + Œ≥Œ≤ÃÇ'CŒ≤ÃÇ for Œ≤ÃÇ as a function of Œ≥::

            (Œ≥C + œÜ(X)'SSœÜ(X)) Œ≤ÃÇ = œÜ(X)'S Sy
            (Œ≥ùïÄ + C‚Åª¬πœÜ(X)'SSœÜ(X)) Œ≤ÃÇ = C‚Åª¬πœÜ(X)'S Sy
            (Œ≥QQ‚Åª¬π + QŒõQ‚Åª¬π) Œ≤ÃÇ = C‚Åª¬πœÜ(X)'S Sy  where  œÜ(X)'SSœÜ(X)Q = CQŒõ
            Œ≤ÃÇ = Q(Œ≥ùïÄ + Œõ)‚Åª¬πQ‚Åª¬π C‚Åª¬πœÜ(X)'S Sy

        The entries of Œ≤ÃÇ are rational polynomials of Œ≥: Q diag([r·µ¢(Œ≥)]·µ¢) Q‚Åª¬π C‚Åª¬πœÜ(X)'SSy. The
        unweighted leave-one-out residuals e‚ÅΩÀ°·µí·µí‚Åæ can be derived by analogy to [1]::

            e·µ¢‚ÅΩÀ°·µí·µí‚Åæ := (œÜ(X)·µ¢Œ≤ÃÇ - y·µ¢) / (1 - h·µ¢)  where  œÜ(X)·µ¢ is the i'th row of œÜ(X)
            h·µ¢ := s·µ¢œÜ(X)·µ¢ (Œ≥C + œÜ(X)'SSœÜ(X))‚Åª¬π s·µ¢œÜ(X)·µ¢'
                = s·µ¢œÜ(X)·µ¢ Q(Œ≥ùïÄ + Œõ)‚Åª¬πQ‚Åª¬πC‚Åª¬π    s·µ¢œÜ(X)·µ¢'

        The entries of h·µ¢ are also rational polynomials of Œ≥: s·µ¢œÜ(X)·µ¢Q diag([r·µ¢(Œ≥)]·µ¢) Q‚Åª¬πC‚Åª¬πœÜ(X)·µ¢'.

        We find the Œ≥ that optimises the weighted mean absolute leave-one-out error s'|e‚ÅΩÀ°·µí·µí‚Åæ| by
        sampling sufficient Œ≥s and picking the best one.

        References
        ----------
        [1] https://robjhyndman.com/hyndsight/loocv-linear-models/
        """
        # Normalise the sample weights.
        s = s / np.sum(s)
        # Compute the GEVD œÜ(X)'SSœÜ(X)Q = CQŒõ so that QŒõQ‚Åª¬π = C‚Åª¬πœÜ(X)'SSœÜ(X).
        SœÜ = s[:, np.newaxis] * œÜ
        A = SœÜ.conj().T @ SœÜ
        A = (A + A.conj().T) / 2  # Ensure A is fully Hermitian.
        c = np.diag(C)
        C_is_diagonal = np.all(np.diag(c) == C)
        C = C / np.mean(np.abs(c)) / œÜ.size  # Normalise C.
        c = c / np.mean(np.abs(c)) / œÜ.size  # Normalise c.
        if C_is_diagonal:
            Œª, Q = eigh((1 / c[:, np.newaxis]) * A)  # Scipy's eigh is faster for complex matrices.
            CQ_inv = Q.conj().T * (1 / c[np.newaxis, :])
        else:
            Œª, Q = eigh(a=A, b=C)
            CQ_lu = lu_factor(C @ Q)
        # Compute the optimal parameters Œ≤ÃÇ = Q(Œ≥I + Œõ)‚Åª¬πQ‚Åª¬πC‚Åª¬πœÜ(X)'SSy as a function of Œ≥. We can
        # evaluate Œ≤ÃÇ(Œ≥) as Œ≤ÃÇ @ (1 / (Œ≥ + Œª)) for a given Œ≥.
        œÜSTSy = SœÜ.conj().T @ (s * y)
        Œ≤ÃÇ = (
            Q * (CQ_inv @ œÜSTSy)[np.newaxis, :]
            if C_is_diagonal
            else Q * lu_solve(CQ_lu, œÜSTSy)[np.newaxis, :]
        )
        # Compute part of the leave-one-out residual numerator s·µ¢(y·µ¢ - œÜ(X)·µ¢Œ≤ÃÇ).
        œÜŒ≤ÃÇ = np.real(œÜ @ Œ≤ÃÇ)
        # Compute the leverage part h·µ¢ of the leave-one-out residual denominator 1 - h·µ¢.
        h = (
            np.real(SœÜ @ Q * (CQ_inv @ SœÜ.conj().T).T)
            if C_is_diagonal
            else np.real(SœÜ @ Q * lu_solve(CQ_lu, SœÜ.conj().T).T)
        )
        # After np.real, arrays are _neither_ C nor F contiguous, which destroys performance.
        œÜŒ≤ÃÇ = np.ascontiguousarray(œÜŒ≤ÃÇ)
        h = np.ascontiguousarray(h)
        # Evaluate the unweighted leave-one-out residuals for a set of Œ≥s with two matrix-matrix
        # products and pick the best solution.
        self.Œ≥s_: FloatVector[F] = np.logspace(np.log10(1e-6), np.log10(20), 1024, dtype=y.dtype)
        rŒ≥ = 1 / (self.Œ≥s_[np.newaxis, :] + Œª[:, np.newaxis])
        with np.errstate(divide="ignore", invalid="ignore"):
            loo_residuals = (œÜŒ≤ÃÇ @ rŒ≥ - y[:, np.newaxis]) / (1 - h @ rŒ≥)
            ≈∑_loo = y[:, np.newaxis] + loo_residuals
        # In the case of binary classification, clip overly positive and overly negative
        # predictions' residuals to 0 when the labels are positive and negative, respectively.
        if self._estimator_type == "classifier":
            loo_residuals[(y > 0)[:, np.newaxis] & (loo_residuals > 0)] = 0
            loo_residuals[(y < 0)[:, np.newaxis] & (loo_residuals < 0)] = 0
        # Select Œ≥ that minimises the number of LOO misclassifications, the degree to which
        # LOO instances are misclassified, and the weighted absolute LOO error.
        self.loo_errors_Œ≥s_ = s @ np.abs(loo_residuals)
        optimum = np.argmin(
            s @ (np.abs(loo_residuals) >= 1)
            + s @ np.maximum(0, np.abs(loo_residuals) - 1)
            + self.loo_errors_Œ≥s_
            if self._estimator_type == "classifier"
            else self.loo_errors_Œ≥s_
        )
        # Store the leave-one-out residuals, leverage, error, and score.
        self.loo_residuals_ = loo_residuals[:, optimum]
        self.loo_≈∑_ = y + self.loo_residuals_
        self.loo_leverage_ = h @ rŒ≥[:, optimum]
        self.loo_error_ = self.loo_errors_Œ≥s_[optimum]
        if self._estimator_type == "classifier":
            self.loo_score_ = accuracy_score(y, np.sign(≈∑_loo[:, optimum]), sample_weight=s)
        elif self._estimator_type == "regressor":
            self.loo_score_ = r2_score(y, ≈∑_loo[:, optimum], sample_weight=s)
        Œ≤ÃÇ, Œ≥ = Œ≤ÃÇ @ rŒ≥[:, optimum], self.Œ≥s_[optimum]
        # Resolve the linear system for better accuracy.
        self.L_ = cho_factor(Œ≥ * C + A)
        Œ≤ÃÇ = cho_solve(self.L_, œÜSTSy)
        self.residuals_ = np.real(œÜ @ Œ≤ÃÇ) - y
        if self._estimator_type == "classifier":
            self.residuals_[(y > 0) & (self.residuals_ > 0)] = 0
            self.residuals_[(y < 0) & (self.residuals_ < 0)] = 0
        # Compute the leave-one-out predictive standard deviation with the Sherman-Morrison formula.
        œÉ2 = np.real(np.sum(œÜ * cho_solve(self.L_, œÜ.conj().T).T, axis=1))
        œÉ2 = np.ascontiguousarray(œÉ2)
        loo_œÉ2 = œÉ2 + (s * œÉ2) ** 2 / (1 - self.loo_leverage_)
        self.loo_std_ = np.sqrt(loo_œÉ2)
        # TODO: Print warning if optimal Œ≥ is found at the edge.
        return Œ≤ÃÇ, Œ≥

    def _optimize_Œ±ÃÇ_Œ≥(
        self,
        X: FloatMatrix[F],
        y: FloatVector[F],
        s: FloatVector[F],
        œÅ: float = 1.0,
    ) -> tuple[FloatVector[F], float]:
        """Find the dual solution to argmin ‚Ñí(e,Œ≤ÃÇ,b,Œ±ÃÇ).

        The Lagrangian is defined as::

            ‚Ñí(e,Œ≤ÃÇ,b,Œ±ÃÇ) := 1/(2Œ≥œÅ) e'S¬≤e + 1/2 (Œ≤ÃÇ'Œ≤ÃÇ + b¬≤) + (1-œÅ)/(2œÅ) Œ±ÃÇ'CŒ±ÃÇ - Œ±ÃÇ'(œÜ(X)Œ≤ÃÇ + b - y - e)

        where Œ≥ determines the weight of the regularisation terms Œ≤ÃÇ'Œ≤ÃÇ and Œ±ÃÇ'CŒ±ÃÇ, which maximise the
        margin and minimise the complexity of the prediction surface, respectively, and œÅ determines
        the trade-off between these two regularisation terms. The residuals e are defined as
        e := œÜ(X)Œ≤ÃÇ + b - y.

        There are two differences w.r.t. the classic LS-SVM formulation: we regularise b in addition
        to Œ≤ÃÇ, and we add a regularisation term for the complexity of the prediction surface of the
        form Œ±ÃÇ'CŒ±ÃÇ. Furthermore, we assume that S = diag(s) and that C is symmetric.

        Setting the gradient of the Lagrangian to zero yields::

            ‚àÇ‚Ñí/‚àÇe = 1/(Œ≥œÅ) S¬≤e + Œ±ÃÇ = 0 => e = -Œ≥œÅ S‚Åª¬≤Œ±ÃÇ
            ‚àÇ‚Ñí/‚àÇŒ≤ÃÇ = Œ≤ÃÇ - œÜ(X)'Œ±ÃÇ = 0 => Œ≤ÃÇ = œÜ(X)'Œ±ÃÇ
            ‚àÇ‚Ñí/‚àÇb = b - 1'Œ±ÃÇ = 0 => b = 1'Œ±ÃÇ
            ‚àÇ‚Ñí/‚àÇŒ±ÃÇ = (1-œÅ)/œÅ CŒ±ÃÇ - œÜ(X)Œ≤ÃÇ - b + y + e = 0 => [œÜ(X)œÜ(X)' + 11' - (1-œÅ)/œÅ C + Œ≥œÅS‚Åª¬≤]Œ±ÃÇ = y

        Let K := œÜ(X)œÜ(X)' + 11' - (1-œÅ)/œÅ C, then we can solve for Œ±ÃÇ(Œ≥)::

            (Œ≥œÅS‚Åª¬≤ + K) Œ±ÃÇ = y  and  ≈∑(x) := k(x, X)Œ±ÃÇ + b
            S‚Åª¬π(Œ≥œÅùïÄ + SKS)S‚Åª¬π Œ±ÃÇ = y
            S‚Åª¬π(Œ≥œÅQQ‚Åª¬π + QŒõQ‚Åª¬π)S‚Åª¬π Œ±ÃÇ = y  where  SKS Q = QŒõ
            Œ±ÃÇ = SQ(Œ≥œÅùïÄ + Œõ)‚Åª¬πQ‚Åª¬πS y

        The entries of Œ±ÃÇ are rational polynomials of Œ≥: Q diag([r·µ¢(Œ≥)]·µ¢) Q‚Åª¬πD‚Åª¬π y.

        Next, we derive the unweighted leave-one-out residuals e‚ÅΩÀ°·µí·µí‚Åæ by analogy to [1]. First, we
        define F := œÜ(X)œÜ(X)' + 11', G := -(1-œÅ)/œÅ C + Œ≥œÅS‚Åª¬≤, and H := (F + G)‚Åª¬π so that::

            (F + G) Œ±ÃÇ = y
            [f‚ÇÅ‚ÇÅ+g‚ÇÅ‚ÇÅ f‚ÇÅ'+g‚ÇÅ'; f‚ÇÅ+g‚ÇÅ F‚ÇÅ+G‚ÇÅ] Œ±ÃÇ = y
            Œ±ÃÇ = Hy = [h‚ÇÅ‚ÇÅ h‚ÇÅ'; h‚ÇÅ H‚ÇÅ] y
            h‚ÇÅ‚ÇÅ := 1/(f‚ÇÅ‚ÇÅ+g‚ÇÅ‚ÇÅ - (f‚ÇÅ'+g‚ÇÅ')(F‚ÇÅ+G‚ÇÅ)‚Åª¬π(f‚ÇÅ+g‚ÇÅ))
            h‚ÇÅ  := -h‚ÇÅ‚ÇÅ(F‚ÇÅ+G‚ÇÅ)‚Åª¬π(f‚ÇÅ+g‚ÇÅ)
            ≈∑‚ÇÅ‚ÅΩ‚Åª¬π‚Åæ := f‚ÇÅ' Œ±ÃÇ‚ÅΩ‚Åª¬π‚Åæ
                   = f‚ÇÅ' (F‚ÇÅ+G‚ÇÅ)‚Åª¬π y‚ÅΩ‚Åª¬π‚Åæ
                   = f‚ÇÅ' (F‚ÇÅ+G‚ÇÅ)‚Åª¬π [f‚ÇÅ+g‚ÇÅ F‚ÇÅ+G‚ÇÅ] Œ±ÃÇ
                   = f‚ÇÅ'[-h‚ÇÅ/h‚ÇÅ‚ÇÅ ùïÄ] Œ±ÃÇ
            y‚ÇÅ = [f‚ÇÅ‚ÇÅ+g‚ÇÅ‚ÇÅ f‚ÇÅ'+g‚ÇÅ'] Œ±ÃÇ
            e‚ÇÅ‚ÅΩÀ°·µí·µí‚Åæ := ≈∑‚ÇÅ‚ÅΩ‚Åª¬π‚Åæ - y‚ÇÅ = f‚ÇÅ'[-h‚ÇÅ/h‚ÇÅ‚ÇÅ ùïÄ] Œ±ÃÇ - y‚ÇÅ

        We find the Œ≥ that optimises the weighted mean absolute leave-one-out error s'|e‚ÅΩÀ°·µí·µí‚Åæ| by
        sampling sufficient Œ≥s and picking the best one.

        References
        ----------
        [1] http://theoval.cmp.uea.ac.uk/publications/pdf/ijcnn2006a.pdf
        """
        # Normalise the sample weights.
        s = s / np.sum(s)
        sn = s / np.median(np.abs(s))
        # Construct the regularisation matrix C that penalises the complexity of the prediction
        # surface. TODO: Document the derivation of this term.
        gamma = 0.5
        C = np.sqrt(rbf_kernel(X, gamma=gamma)) * (
            1 - euclidean_distances(X, squared=True) * (gamma / X.shape[1])
        )
        # Construct F := œÜ(X)œÜ(X)' + 11'.
        F = rbf_kernel(X, gamma=0.5) + np.ones(X.shape[0], dtype=X.dtype)
        # Construct D‚Åª¬πK := 1/œÅ S¬≤ [œÜ(X)œÜ(X)' + 11' - (1-œÅ)/œÅ C].
        K = F - (1 - œÅ) / œÅ * C
        # Compute the EVD SKS Q = QŒõ.
        Œª, Q = np.linalg.eigh(sn[:, np.newaxis] * K * sn[np.newaxis, :])
        # Compute the optimal parameters aÃÇ = SQ(Œ≥œÅI + Œõ)‚Åª¬πQ‚Åª¬πS y as a function of Œ≥. We can evaluate
        # aÃÇ(Œ≥) as Œ±ÃÇ @ (1 / (Œ≥œÅ + Œª)) for a given Œ≥.
        Œ±ÃÇ = (sn[:, np.newaxis] * Q) * (Q.conj().T @ (sn * y))[np.newaxis, :]
        # Evaluate the unweighted leave-one-out residuals for a set of Œ≥s and pick the best one.
        self.Œ≥s_ = np.logspace(np.log10(1e-6), np.log10(20), 128, dtype=X.dtype)
        # Compute the leave-one-out predictions ≈∑‚ÇÅ‚ÅΩ‚Åª¬π‚Åæ = f‚ÇÅ'[-h‚ÇÅ/h‚ÇÅ‚ÇÅ ùïÄ] Œ±ÃÇ.
        H_loo = np.einsum(  # Compute H := SQ(Œ≥œÅI + Œõ)‚Åª¬πQ‚Åª¬πS as a function of Œ≥.
            "ij,gj,jk->igk",
            sn[:, np.newaxis] * Q,
            1 / (self.Œ≥s_[:, np.newaxis] * œÅ + Œª[np.newaxis, :]),
            Q.conj().T * sn[np.newaxis, :],
            optimize="optimal",
        )
        for g in range(H_loo.shape[1]):
            h = np.diag(H_loo[:, g, :]).copy()
            h[h == 0] = np.finfo(X.dtype).eps  # Avoid division by zero.
            H_loo[:, g, :] = H_loo[:, g, :] / -h[:, np.newaxis]
        F_loo = F.copy()
        np.fill_diagonal(F_loo, 0)
        Œ±ÃÇ_loo = Œ±ÃÇ @ (1 / (self.Œ≥s_[np.newaxis, :] * œÅ + Œª[:, np.newaxis]))
        ≈∑_loo = np.sum(F_loo[:, np.newaxis, :] * H_loo, axis=2) * Œ±ÃÇ_loo + F_loo @ Œ±ÃÇ_loo
        loo_residuals = ≈∑_loo - y[:, np.newaxis]
        # In the case of binary classification, clip overly positive and overly negative
        # predictions' residuals to 0 when the labels are positive and negative, respectively.
        if self._estimator_type == "classifier":
            loo_residuals[(y > 0)[:, np.newaxis] & (loo_residuals > 0)] = 0
            loo_residuals[(y < 0)[:, np.newaxis] & (loo_residuals < 0)] = 0
        # Select Œ≥ that minimises the number of LOO misclassifications, the degree to which
        # LOO instances are misclassified, and the weighted absolute LOO error.
        self.loo_errors_Œ≥s_ = s @ np.abs(loo_residuals)
        optimum = np.argmin(
            s @ (np.abs(loo_residuals) >= 1)
            + s @ np.maximum(0, np.abs(loo_residuals) - 1)
            + self.loo_errors_Œ≥s_
            if self._estimator_type == "classifier"
            else self.loo_errors_Œ≥s_
        )
        # Store the leave-one-out residuals, leverage, error, and score.
        self.loo_residuals_ = loo_residuals[:, optimum]
        self.loo_≈∑_ = y + self.loo_residuals_
        self.loo_error_ = self.loo_errors_Œ≥s_[optimum]
        if self._estimator_type == "classifier":
            self.loo_score_ = accuracy_score(y, np.sign(≈∑_loo[:, optimum]), sample_weight=s)
        elif self._estimator_type == "regressor":
            self.loo_score_ = r2_score(y, ≈∑_loo[:, optimum], sample_weight=s)
        Œ±ÃÇ, Œ≥ = Œ±ÃÇ_loo[:, optimum], self.Œ≥s_[optimum]
        # Resolve the linear system for better accuracy.
        self.L_ = cho_factor(Œ≥ * œÅ * np.diag(sn**-2) + K)
        Œ±ÃÇ = cho_solve(self.L_, y)
        self.residuals_ = F @ Œ±ÃÇ - y
        if self._estimator_type == "classifier":
            self.residuals_[(y > 0) & (self.residuals_ > 0)] = 0
            self.residuals_[(y < 0) & (self.residuals_ < 0)] = 0
        # Compute the leave-one-out predictive standard deviation.
        # TODO: Apply a leave-one-out correction.
        K = rbf_kernel(X, gamma=0.5)
        œÉ2 = 1.0 - np.sum(K * cho_solve(self.L_, K.T).T, axis=1)
        self.loo_std_ = np.sqrt(œÉ2)
        # TODO: Print warning if optimal Œ≥ is found at the edge.
        return Œ±ÃÇ, Œ≥

    def fit(
        self,
        X: "FloatMatrix[F] | pd.DataFrame",
        y: "GenericVector | pd.Series",
        sample_weight: "FloatVector[F] | pd.Series | None" = None,
    ) -> "NeoLSSVM":
        """Fit this predictor."""
        # Remove singleton dimensions from y and validate input.
        X, y = check_X_y(X, y, dtype=(np.float64, np.float32), ensure_min_samples=2)
        y = np.ravel(np.asarray(y))
        self.n_features_in_ = X.shape[1]
        # Store the target's dtype for prediction.
        self.y_dtype_: npt.DTypeLike = y.dtype
        # Use uniform sample weights if none are provided.
        sample_weight_ = (
            np.ones(y.shape, X.dtype)
            if sample_weight is None
            else np.ravel(np.asarray(sample_weight)).astype(X.dtype)
        )
        check_consistent_length(y, sample_weight_)
        # Learn the type of task from the target. Set the `_estimator_type` and store the `classes_`
        # attribute if this is a classification task [1].
        # [1] https://scikit-learn.org/stable/glossary.html#term-classifiers
        y_: FloatVector[F]
        unique_y = np.unique(y)
        inferred_estimator_type = None
        if len(unique_y) == 2:  # noqa: PLR2004
            inferred_estimator_type = "classifier"
        elif (
            np.issubdtype(y.dtype, np.number)
            or np.issubdtype(y.dtype, np.datetime64)
            or np.issubdtype(y.dtype, np.timedelta64)
        ):
            inferred_estimator_type = "regressor"
        self._estimator_type: str | None = (
            inferred_estimator_type if self.estimator_type == "auto" else self.estimator_type
        )
        if self._estimator_type == "classifier":
            self.classes_: GenericVector = unique_y
            negatives = y == self.classes_[0]
            y_ = np.ones(y.shape, dtype=X.dtype)
            y_[negatives] = -1
        elif self._estimator_type == "regressor":
            y_ = y.astype(X.dtype)
        else:
            message = "Target type not supported"
            raise ValueError(message)
        # Determine whether we want to solve this in the primal or dual space.
        self.dual_ = X.shape[0] <= 1024 if self.dual == "auto" else self.dual  # noqa: PLR2004
        self.primal_ = not self.dual_
        # Learn an optimal distance metric for the primal or dual space and apply it to the feature
        # matrix X.
        if self.primal_:
            self.primal_feature_map_ = clone(
                OrthogonalRandomFourierFeatures()
                if self.primal_feature_map == "auto"
                else self.primal_feature_map
            )
            self.primal_feature_map_.fit(X, y_, sample_weight_)
            œÜ = self.primal_feature_map_.transform(X)
        else:
            nz_weight = sample_weight_ > 0
            X, y_, sample_weight_ = X[nz_weight], y_[nz_weight], sample_weight_[nz_weight]
            self.dual_feature_map_ = clone(
                AffineSeparator() if self.dual_feature_map == "auto" else self.dual_feature_map
            )
            self.dual_feature_map_.fit(X, y_, sample_weight_)
            self.X_ = self.dual_feature_map_.transform(X)
        # Solve the primal or dual system. We optimise the following sub-objectives for the weights
        # Œ≤ÃÇ and hyperparameter Œ≥:
        #   1. Minimal mean squared error on training set: ||S(y - œÜ(X)Œ≤ÃÇ)||¬≤
        #   2. Maximal margin: ||Œ≤ÃÇ||¬≤.
        #   3. Minimal complexity of the prediction surface: ‚à´||‚àá‚ÇìœÜ(x)'Œ≤ÃÇ||¬≤dx
        if self.primal_:
            C = self.primal_feature_map_.complexity_matrix.astype(œÜ.dtype)
            self.Œ≤ÃÇ_, self.Œ≥_ = self._optimize_Œ≤ÃÇ_Œ≥(œÜ=œÜ, y=y_, s=sample_weight_, C=C)
        else:
            self.Œ±ÃÇ_, self.Œ≥_ = self._optimize_Œ±ÃÇ_Œ≥(X=self.X_, y=y_, s=sample_weight_)
        # Calibrate probabilities with isotonic regression on the leave-one-out predictions.
        if self._estimator_type == "classifier":
            self.predict_proba_calibrator_ = IsotonicRegression(
                out_of_bounds="clip", y_min=0, y_max=1, increasing=True
            )
            target = np.zeros_like(y_)
            target[y_ == np.max(y_)] = 1.0
            self.predict_proba_calibrator_.fit(self.loo_≈∑_, target, sample_weight_)
        # Split the leave-one-out predictions into two conformal calibration levels.
        (
            self.nonconformity_calib_l1_,
            self.nonconformity_calib_l2_,
            self.≈∑_calib_l1_,
            self.≈∑_calib_l2_,
            self.residuals_calib_l1_,
            self.residuals_calib_l2_,
            self.sample_weight_calib_l1_,
            self.sample_weight_calib_l2_,
        ) = train_test_split(
            self.loo_std_,
            self.loo_≈∑_,
            self.loo_residuals_,
            sample_weight_,
            train_size=min(1440, max(1024, (X.shape[0] * 2) // 3), X.shape[0] - 1),
            random_state=self.random_state,
        )
        # Lazily fit level 1 conformal predictors as coherent linear quantile regression models that
        # predict quantiles of the (relative) residuals given the nonconformity estimates, and
        # level 2 conformal biases.
        self.conformal_l1_: dict[str, dict[tuple[float, ...], CoherentLinearQuantileRegressor]] = {
            "Œî≈∑": {},
            "Œî≈∑/≈∑": {},
        }
        self.conformal_l2_: dict[str, dict[tuple[float, ...], FloatVector[F]]] = {
            "Œî≈∑": {},
            "Œî≈∑/≈∑": {},
        }
        return self

    @overload
    def predict_std(self, X: FloatMatrix[F]) -> FloatVector[F]:
        ...

    @overload
    def predict_std(self, X: "pd.DataFrame") -> "pd.Series":
        ...

    def predict_std(self, X: "FloatMatrix[F] | pd.DataFrame") -> "FloatVector[F] | pd.Series":
        """Compute a Bayesian estimate of the standard deviation of the predictive distribution.

        Note that the Bayesian estimate of the predictive standard deviation is based on several
        assumptions and is not calibrated. As a consequence, it is unlikely to be an accurate
        estimation of the standard deviation as is. However, it may be useful as a nonconformity
        estimate for conformally calibrated prediction of quantiles or intervals.
        """
        # Estimate the predictive variance of the predictive distribution p(≈∑(x)).
        check_is_fitted(self)
        X, X_df = check_array(X, dtype=(np.float64, np.float32)), X
        œÉ2: FloatVector[F]
        if self.primal_:
            # If Œ≤ÃÇ := (LL')‚Åª¬π y* and cov(y*) := LL', then cov(Œ≤ÃÇ) = cov((LL')‚Åª¬π y*) = (LL')‚Åª¬π
            # assuming ùîº(Œ≤ÃÇ) = 0. It follows that cov(≈∑(x)) = cov(œÜ(x)'Œ≤ÃÇ) = œÜ(x)'(LL')‚Åª¬πœÜ(x).
            œÜH = cast(KernelApproximatingFeatureMap, self.primal_feature_map_).transform(X)
            œÉ2 = np.real(np.sum(œÜH * cho_solve(self.L_, œÜH.conj().T).T, axis=1))
            œÉ2 = np.ascontiguousarray(œÉ2)
        else:
            # Compute the cov(≈∑(x)) as K(x, x) ‚àí K(x, X) (LL')‚Åª¬π K(X, x).
            # TODO: Document derivation.
            X = cast(AffineFeatureMap, self.dual_feature_map_).transform(X)
            K = rbf_kernel(X, self.X_, gamma=0.5)
            œÉ2 = 1.0 - np.sum(K * cho_solve(self.L_, K.T).T, axis=1)
        # Convert the variance to a standard deviation.
        œÉ = np.sqrt(œÉ2)
        # Convert to a pandas Series if the input was a pandas DataFrame.
        if hasattr(X_df, "dtypes") and hasattr(X_df, "index"):
            try:
                import pandas as pd
            except ImportError:
                pass
            else:
                œÉ_series = pd.Series(œÉ, index=X_df.index)
                return œÉ_series
        return œÉ

    def _lazily_fit_conformal_predictor(
        self, target_type: str, quantiles: npt.ArrayLike
    ) -> tuple[CoherentLinearQuantileRegressor, FloatVector[F]]:
        """Lazily fit a conformal predictor for a given array of quantiles."""
        quantiles = np.asarray(quantiles)
        quantiles_tuple = tuple(quantiles)
        if quantiles_tuple in self.conformal_l1_[target_type]:
            # Retrieve level 1 and level 2.
            cqr_l1 = self.conformal_l1_[target_type][quantiles_tuple]
            bias_l2 = self.conformal_l2_[target_type][quantiles_tuple]
        else:
            # Fit level 1: a coherent quantile regressor that predicts quantiles of the (relative)
            # residuals.
            eps = np.finfo(self.≈∑_calib_l1_.dtype).eps
            abs_≈∑_calib_l1 = np.maximum(np.abs(self.≈∑_calib_l1_), eps)
            X_cqr_l1 = self.nonconformity_calib_l1_[:, np.newaxis]
            if self._estimator_type == "regressor":
                X_cqr_l1 = np.hstack([X_cqr_l1, np.abs(self.≈∑_calib_l1_[:, np.newaxis])])
            y_cqr_l1 = -self.residuals_calib_l1_ / (abs_≈∑_calib_l1 if "/≈∑" in target_type else 1)
            cqr_l1 = CoherentLinearQuantileRegressor(quantiles=quantiles)
            cqr_l1.fit(X_cqr_l1, y_cqr_l1, sample_weight=self.sample_weight_calib_l1_)
            self.conformal_l1_[target_type][quantiles_tuple] = cqr_l1
            # Fit level 2: a per-quantile conformal bias on top of the level 1 conformal quantile
            # predictions of the (relative) residuals.
            bias_l2 = np.zeros(quantiles.shape, dtype=self.≈∑_calib_l1_.dtype)
            if len(self.≈∑_calib_l2_) >= 128:  # noqa: PLR2004
                abs_≈∑_calib_l2 = np.maximum(np.abs(self.≈∑_calib_l2_), eps)
                X_cqr_l2 = self.nonconformity_calib_l2_[:, np.newaxis]
                if self._estimator_type == "regressor":
                    X_cqr_l2 = np.hstack([X_cqr_l2, np.abs(self.≈∑_calib_l2_[:, np.newaxis])])
                y_cqr_l2 = -self.residuals_calib_l2_ / (
                    abs_≈∑_calib_l2 if "/≈∑" in target_type else 1
                )
                Œî≈∑_calib_l2_quantiles = cqr_l1.predict(X_cqr_l2)
                intercept_clip = cqr_l1.intercept_clip(
                    np.vstack([X_cqr_l1, X_cqr_l2]), np.hstack([y_cqr_l1, y_cqr_l2])
                )
                for j, quantile in enumerate(quantiles):
                    # Clip the bias to retain quantile coherence.
                    # TODO: Use a weighted quantile.
                    intercept_l2 = np.quantile(y_cqr_l2 - Œî≈∑_calib_l2_quantiles[:, j], quantile)
                    bias_l2[j] = np.clip(intercept_l2, intercept_clip[0, j], intercept_clip[1, j])
            self.conformal_l2_[target_type][quantiles_tuple] = bias_l2
        return cqr_l1, bias_l2  # type: ignore[return-value]

    @overload
    def predict_quantiles(
        self,
        X: FloatMatrix[F],
        *,
        quantiles: npt.ArrayLike = (0.025, 0.5, 0.975),
        priority: Literal["accuracy", "coverage"] = "accuracy",
    ) -> FloatMatrix[F] | FloatTensor[F]:
        ...

    @overload
    def predict_quantiles(
        self,
        X: "pd.DataFrame",
        *,
        quantiles: npt.ArrayLike = (0.025, 0.5, 0.975),
        priority: Literal["accuracy", "coverage"] = "accuracy",
    ) -> "pd.DataFrame":
        ...

    def predict_quantiles(
        self,
        X: "FloatMatrix[F] | pd.DataFrame",
        *,
        quantiles: npt.ArrayLike = (0.025, 0.5, 0.975),
        priority: Literal["accuracy", "coverage"] = "accuracy",
    ) -> "FloatMatrix[F] | FloatTensor[F] | pd.DataFrame":
        """Predict conformally calibrated quantiles."""
        # Predict the absolute and relative quantiles.
        check_is_fitted(self)
        X, X_df = check_array(X, dtype=(np.float64, np.float32)), X
        ≈∑ = self.decision_function(X)
        X_cqr = self.predict_std(X)[:, np.newaxis]
        if self._estimator_type == "regressor":
            X_cqr = np.hstack([X_cqr, np.abs(≈∑[:, np.newaxis])])
        cqr_abs, bias_abs = self._lazily_fit_conformal_predictor("Œî≈∑", quantiles)
        cqr_rel, bias_rel = self._lazily_fit_conformal_predictor("Œî≈∑/≈∑", quantiles)
        if priority == "coverage":  # Only allow quantile expansion when the priority is coverage.
            center = 0.5
            quantiles = np.asarray(quantiles)
            bias_abs[center <= quantiles] = np.maximum(bias_abs[center <= quantiles], 0)
            bias_abs[quantiles <= center] = np.minimum(bias_abs[quantiles <= center], 0)
            bias_rel[center <= quantiles] = np.maximum(bias_rel[center <= quantiles], 0)
            bias_rel[quantiles <= center] = np.minimum(bias_rel[quantiles <= center], 0)
        Œî≈∑_quantiles = np.dstack(
            [
                cqr_abs.predict(X_cqr) + bias_abs[np.newaxis, :],
                np.abs(≈∑[:, np.newaxis]) * (cqr_rel.predict(X_cqr) + bias_rel[np.newaxis, :]),
            ]
        )
        # Choose between the the absolute and relative quantiles for each example in order to
        # minimise the dispersion of the predicted quantiles.
        dispersion = np.std(Œî≈∑_quantiles, axis=1)
        Œî≈∑_quantiles = Œî≈∑_quantiles[
            np.arange(Œî≈∑_quantiles.shape[0]), :, np.argmin(dispersion, axis=-1)
        ]
        ≈∑_quantiles: FloatMatrix[F] = ≈∑[:, np.newaxis] + Œî≈∑_quantiles
        # In case of classification, convert the decision function values to an
        # example x quantile x class probability tensor.
        if self._estimator_type == "classifier":
            ≈∑_quantiles = np.hstack(
                [
                    self.predict_proba_calibrator_.transform(≈∑_quantiles[:, j])[:, np.newaxis]
                    for j in range(≈∑_quantiles.shape[1])
                ]
            )
            ≈∑_quantiles = np.dstack([1 - ≈∑_quantiles[:, ::-1], ≈∑_quantiles])
        # In case of regression, convert the prediction function values to the target dtype.
        if self._estimator_type == "regressor" and not np.issubdtype(self.y_dtype_, np.integer):
            ≈∑_quantiles = ≈∑_quantiles.astype(self.y_dtype_)
        # Convert ≈∑_quantiles to a pandas DataFrame if X is a pandas DataFrame.
        if hasattr(X_df, "dtypes") and hasattr(X_df, "index"):
            try:
                import pandas as pd
            except ImportError:
                pass
            else:
                if self._estimator_type == "regressor":
                    ≈∑_quantiles_df = pd.DataFrame(≈∑_quantiles, index=X_df.index, columns=quantiles)
                else:
                    neg_df = pd.DataFrame(≈∑_quantiles[:, :, 0], index=X_df.index, columns=quantiles)
                    pos_df = pd.DataFrame(≈∑_quantiles[:, :, 1], index=X_df.index, columns=quantiles)
                    ≈∑_quantiles_df = pd.concat(
                        [neg_df, pos_df],
                        axis=0,
                        keys=self.classes_,
                        names=["class", X_df.index.name],
                    )
                ≈∑_quantiles_df.columns.name = "quantile"
                return ≈∑_quantiles_df
        return ≈∑_quantiles

    @overload
    def predict_interval(
        self, X: FloatMatrix[F], *, coverage: float = 0.95
    ) -> FloatMatrix[F] | FloatTensor[F]:
        ...

    @overload
    def predict_interval(self, X: "pd.DataFrame", *, coverage: float = 0.95) -> "pd.DataFrame":
        ...

    def predict_interval(
        self, X: "FloatMatrix[F] | pd.DataFrame", *, coverage: float = 0.95
    ) -> "FloatMatrix[F] | FloatTensor[F] | pd.DataFrame":
        """Predict conformally calibrated intervals."""
        # Convert the coverage probability to lower and upper quantiles.
        lb = (1 - coverage) / 2
        ub = 1 - lb
        # Compute the prediction interval with predict_quantiles.
        ≈∑_quantiles = self.predict_quantiles(X, quantiles=(lb, ub), priority="coverage")
        return ≈∑_quantiles

    @overload
    def decision_function(self, X: FloatMatrix[F]) -> FloatVector[F]:
        ...

    @overload
    def decision_function(self, X: "pd.DataFrame") -> "pd.Series":
        ...

    def decision_function(self, X: "FloatMatrix[F] | pd.DataFrame") -> "FloatVector[F] | pd.Series":
        """Evaluate the prediction function."""
        # Compute the point predictions ≈∑(X).
        check_is_fitted(self)
        X, X_df = check_array(X, dtype=(np.float64, np.float32)), X
        ≈∑: FloatVector[F]
        if self.primal_:
            # Apply the feature map œÜ and predict as ≈∑(x) := œÜ(x)'Œ≤ÃÇ.
            œÜ = cast(KernelApproximatingFeatureMap, self.primal_feature_map_).transform(X)
            ≈∑ = np.real(œÜ @ self.Œ≤ÃÇ_)
            ≈∑ = np.ascontiguousarray(≈∑)
        else:
            # Apply an affine transformation to X, then predict as ≈∑(x) := k(x, X) Œ±ÃÇ + 1'Œ±ÃÇ.
            X = cast(AffineFeatureMap, self.dual_feature_map_).transform(X)
            K = rbf_kernel(X, self.X_, gamma=0.5)
            b = np.sum(self.Œ±ÃÇ_)
            ≈∑ = K @ self.Œ±ÃÇ_ + b
        # Convert to a pandas Series if the input was a pandas DataFrame.
        if hasattr(X_df, "dtypes") and hasattr(X_df, "index"):
            try:
                import pandas as pd
            except ImportError:
                pass
            else:
                ≈∑_series = pd.Series(≈∑, index=X_df.index)
                return ≈∑_series
        return ≈∑

    @overload
    def predict(
        self, X: FloatMatrix[F], *, coverage: None = None, quantiles: None = None
    ) -> FloatVector[F]:
        ...

    @overload
    def predict(
        self, X: FloatMatrix[F], *, coverage: float, quantiles: None = None
    ) -> FloatMatrix[F]:
        ...

    @overload
    def predict(
        self, X: FloatMatrix[F], *, coverage: None = None, quantiles: npt.ArrayLike
    ) -> FloatMatrix[F]:
        ...

    @overload
    def predict(
        self, X: "pd.DataFrame", *, coverage: None = None, quantiles: None = None
    ) -> "pd.Series":
        ...

    @overload
    def predict(
        self, X: "pd.DataFrame", *, coverage: float, quantiles: None = None
    ) -> "pd.DataFrame":
        ...

    @overload
    def predict(
        self, X: "pd.DataFrame", *, coverage: None = None, quantiles: npt.ArrayLike
    ) -> "pd.DataFrame":
        ...

    def predict(
        self,
        X: "FloatMatrix[F] | pd.DataFrame",
        *,
        coverage: float | None = None,
        quantiles: npt.ArrayLike | None = None,
    ) -> "FloatVector[F] | pd.Series | FloatMatrix[F] | pd.DataFrame":
        """Predict on a given dataset."""
        # Return a prediction interval or quantiles if requested.
        assert coverage is None or quantiles is None
        if coverage is not None:
            ≈∑_interval = self.predict_interval(X, coverage=coverage)
            return ≈∑_interval
        if quantiles is not None:
            ≈∑_quantiles = self.predict_quantiles(X, quantiles=quantiles)
            return ≈∑_quantiles
        # Compute the point predictions ≈∑(X).
        check_is_fitted(self)
        X, X_df = check_array(X, dtype=(np.float64, np.float32)), X
        ≈∑_df = self.decision_function(X)
        if self._estimator_type == "classifier":
            # For binary classification, round to the nearest class label. When the decision
            # function is 0, we assign a negative class label [1].
            # [1] https://scikit-learn.org/stable/glossary.html#term-decision_function
            ≈∑_df = np.sign(≈∑_df)
            ≈∑_df[≈∑_df == 0] = -1
            # Remap to the original class labels.
            ≈∑ = self.classes_[((≈∑_df + 1) // 2).astype(np.intp)]
        elif self._estimator_type == "regressor":
            # The decision function is the point prediction.
            ≈∑ = ≈∑_df
        # Map back to the training target dtype.
        if not np.issubdtype(self.y_dtype_, np.integer):
            ≈∑ = ≈∑.astype(self.y_dtype_)
        # Convert to a pandas Series if the input was a pandas DataFrame.
        if hasattr(X_df, "dtypes") and hasattr(X_df, "index"):
            try:
                import pandas as pd
            except ImportError:
                pass
            else:
                ≈∑_series = pd.Series(≈∑, index=X_df.index)
                return ≈∑_series
        return ≈∑

    @overload
    def predict_proba(self, X: FloatMatrix[F]) -> "FloatVector[F] | FloatMatrix[F]":
        ...

    @overload
    def predict_proba(self, X: "pd.DataFrame") -> "pd.Series | pd.DataFrame":
        ...

    def predict_proba(
        self, X: "FloatMatrix[F] | pd.DataFrame"
    ) -> "FloatVector[F] | FloatMatrix[F] | pd.Series | pd.DataFrame":
        """Predict the class probability or confidence interval."""
        check_is_fitted(self)
        X, X_df = check_array(X, dtype=(np.float64, np.float32)), X
        ≈∑_df = self.decision_function(X)
        if self._estimator_type == "classifier":
            # Return the class probabilities for classification.
            proba_pos = self.predict_proba_calibrator_.transform(≈∑_df)
            proba = np.hstack([1 - proba_pos[:, np.newaxis], proba_pos[:, np.newaxis]])
        else:
            # Map back to the training target dtype unless that would cause loss of precision.
            proba = ≈∑_df
            if not np.issubdtype(self.y_dtype_, np.integer):
                proba = ≈∑_df.astype(self.y_dtype_)
        # Convert proba to a pandas Series or DataFrame if X is a pandas DataFrame.
        if hasattr(X_df, "dtypes") and hasattr(X_df, "index"):
            try:
                import pandas as pd
            except ImportError:
                pass
            else:
                if self._estimator_type == "regressor":
                    return pd.Series(proba, index=X_df.index)
                if self._estimator_type == "classifier":
                    return pd.DataFrame(proba, index=X_df.index, columns=self.classes_)
        return proba

    def score(
        self,
        X: "FloatMatrix[F] | pd.DataFrame",
        y: "GenericVector | pd.Series",
        sample_weight: FloatVector[F] | None = None,
    ) -> float:
        """Compute the accuracy or R¬≤ of this classifier or regressor."""
        ≈∑ = self.predict(X)
        score: float
        if self._estimator_type == "classifier":
            score = accuracy_score(y, ≈∑, sample_weight=sample_weight)
        elif self._estimator_type == "regressor":
            # Cast to a numeric dtype in case the target is a datetime or timedelta.
            score = r2_score(
                y.astype(np.float64), ≈∑.astype(np.float64), sample_weight=sample_weight
            )
        return score

    def _more_tags(self) -> dict[str, Any]:
        # https://scikit-learn.org/stable/developers/develop.html#estimator-tags
        return {"binary_only": True, "requires_y": True}
